---
layout: post
date: 2020-01-01 16:00:00 +0800
title: "Some drafts"
category: draft
---

## systemd

### Q：systemd的一些基础的服务是怎么拉起的？例如`systemd-tmpfiles-setup.service`。

A：通过`/lib/systemd/system/sysinit.target.wants/`目录创建的依赖关系。

## criu

criu能够用于进程的保存和恢复，基于的信息全部来自于内核接口，原声的criu没有内核态代码。

主函数的入口是`criu/crtools.c`，保存的主入口：`cr_dump_tasks`，恢复的主入口：`cr_restore_tasks`。

在保存进程状态前，criu会使用freezer cgroup或者ptrace冻结进程，避免状态不一致。保存一个进程涉及的关键信息很多，例如：信号、内存布局、cgroup等。

**内存恢复：** 调用`prctl(2)`的PR_SET_MM。
**PID的恢复：** 调用`clone(3)`配置PID。这里是如何确保PID不被复用的？

## epoll

epoll的底层机制使用的是`waitqueue（等待队列）`而不是软中断。

关于`wait queue`：<https://www.cnblogs.com/hueyxu/p/13745029.html>、<https://unix.stackexchange.com/questions/268027/how-does-linux-kernel-find-out-which-process-to-wake-up-during-interrupt-handlin>

等待队列一些函数在`wait.c，wait.h`中实现。初始化等待队列头：init_waitqueue_head()，等待队列头用于实现阻塞和唤醒的相关机制。添加到等待队列实际上就是放在对应的等待队列头后面，等待队列头唤醒时，唤醒队列上的其他task。

唤醒等待队列需要调用`wake_up_*()`相关的函数，这种函数一般是在驱动程序、文件系统、网络子系统、定时器中实现。

结合一下定时器的代码呢？<https://void-star.icu/archives/573>，在process_timeout()中会调用到wake_up_process，这个函数实际上是对try_to_wake_up()的封装，try_to_wake_up()会唤醒对应的进程。

epoll实现的文档实现可以参考：<https://www.jxhs.me/2021/04/08/linux%E5%86%85%E6%A0%B8Epoll-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/>

关键点总结：

1. 使用`epoll_create()`会创建一个eventpoll的对象。
2. 在我们通过`epoll_ctl()`添加某个fd进行监控时，内核通过调用`ep_insert()`函数创建一个epitem，并将其插入到eventpoll下由epitem组成的红黑树上。
3. `ep_insert()`中还有一个关键的步骤是调用`tfile->f_op->poll(tfile, &epq.pt)`，这是调用当前添加的fd对应的poll()实现。不同类型的fd有不同类型的实现。下面以tcp_poll为例。
4. tcp_poll的内部实现会将epitem放在fd的等待队列中，如果fd的状态发生变化，唤醒对应的进程。

唤醒对应的函数为：`wake_up_*()`，`wake_up_*()`一般是在驱动或fs目录中实现的，对应下面的第9步？

借epoll的专题写一下网卡接受到数据到进程处理的整个流程。

先看一下ChatGPT怎么说：

1. 数据包到达: 当网卡接收到一个数据包，它会根据预先配置的DMA（Direct Memory Access）将数据直接写入到内存中，并且触发一个硬件中断。
2. 硬件中断: CPU接收到硬件中断信号后，会暂停当前执行的任务，保存其状态，并执行中断服务程序（Interrupt Service Routine, ISR）。在Linux中，每个设备驱动会注册自己的ISR。
3. 中断服务程序: 网卡的ISR会进行一些基本的处理，如确认中断来源、读取状态寄存器、清除中断标志等，然后通常会将实际的数据处理工作推迟到一个软中断或工作队列中去完成，这样可以尽快释放中断处理程序，减少对CPU的占用。
4. NAPI（New API）: 现代Linux网络驱动通常使用NAPI结构来提高性能和减少中断负载。在高负载下，NAPI可以禁用中断，并改用轮询模式来处理网络数据包。
5. 软中断和工作队列: 数据包处理通常在软中断（softirq）或工作队列（workqueue）的上下文中进行。这些机制允许内核延迟处理函数的执行，从而不会阻塞中断服务程序。
6. 内核网络栈处理: 数据包被传递到内核网络栈，这里包括了多层处理，如链路层、网络层（IP层）、传输层（TCP/UDP）等。
7. 套接字缓冲区: 一旦内核确定了数据包属于哪个套接字，它会将数据包放入该套接字的接收缓冲区中。如果缓冲区已满，新的数据包可能会被丢弃，这取决于所使用的协议和套接字的设置。
8. 唤醒等待进程: 如果有用户态的进程正在等待这些数据（例如，通过调用read()或recv()等系统调用），内核会将这些进程从它们的等待队列中唤醒。
9. 系统调用: 用户态的应用程序通过系统调用与内核通信，请求网络数据。常见的系统调用包括read(), recv(), recvfrom(), recvmsg()等。
10. 用户态处理: 用户进程接收到数据后，可以进行相应的处理，如解析应用层协议，处理业务逻辑等。

### epoll和select的区别

||select|epoll|
|-|-|-|
|性能|随着连接数量的增加，性能急剧下降|连接数量增加时，性能没有明显损耗|
|连接数|最多支持默认1024个连接，超过限制需要重新配置宏编译|无限制|
|内在的处理逻辑|线性轮询|回调callback|
|开发难度|低|高|

poll和select的区别其实不大，poll对select的优化在：（1）没有文件描述符数量的限制（2）文件描述符集合从用户空间拷贝到内核空间的开销较小。

select的时间复杂度是O(n)的，而epoll的时间复杂度是O(1)的。在连接数少且十分活跃的情况下，select的性能其实更好一些。

epoll的水平触发和边缘触发：

||水平触发|边缘触发|
|-|-|-|
|差异|只要有数据可读，epoll_wait都会返回事件|只在数据到达时返回一次|
|其他|默认模式|高速模式|
|注意点|无|必须在读取数据时把数据读完|

## crash + vmcore

kdump用于在内核崩溃时捕获内核转储文件(vmcore)，这个文件通常保存在`/var/crash`目录下。使用`crash`工具调试vmcore：`crash /path/to/vmlinux /path/to/vmcore`。

接下来的使用与通过gdb调试用户态的coredump是类似的，常用的命令有：

* bt：打印内核栈，也可以通过`bt <PID>`获取某个进程的栈
* dmesg：获取崩溃时的日志
* dis：反汇编
* mod：查看内核加载的模块列表`mod -L`，查看指定模块的详细信息
* ps：获取进程信息
* file：查看进程打开的文件：`file <PID>`
* task：查看进程的`task_struct`和`thread_info`

另外的定位手段是使用内核的`printk`接口打印日志，四个参数的含义如下。

1. 控制台日志级别：优先级高于该值的消息将被打印至控制台。
2. 缺省的消息日志级别：将用该值来打印没有优先级的消息。
3. 最低的控制台日志级别：控制台日志级别可能被设置的最小值。
4. 缺省的控制台日志级别：控制台日志级别的缺省值。

注意：优先级越高，数值越小。

## buffer + cache

`free`命令能够查看到当前环境上内存详情：

```sh
einsler@debian:~$ free -h
               total        used        free      shared  buff/cache   available
Mem:            15Gi       2.4Gi        10Gi       556Mi       2.7Gi        12Gi
Swap:          976Mi          0B       976Mi
```

这里的`buff/cache`是指缓冲区，区别如下：

* buffer：磁盘IO的缓冲区，用于内存和磁盘的缓冲，将分散的写操作集中，避免磁盘碎片和磁盘的反复寻道，提高系统的性能。
* cache：CPU和内存之间的缓冲，属于文件系统的cache，它会把读取的数据保存起来，如果重新读取时命中就无需再从磁盘获取。

### 如果`buff/cache`占用过高，如何清理？

执行手动清除命令：

```sh
sync
echo 1 > /proc/sys/vm/drop_caches
echo 2 > /proc/sys/vm/drop_caches
echo 3 > /proc/sys/vm/drop_caches
```

## 内存还是内存

如何配置？还是通过sysctl参数：`/proc/sys/vm/min_free_kbytes`。默认情况下，LOW是MIN的1.25倍，HIGH是LOW的1.5倍。

在内核中每个文件都会有一个属于自己的 page cache（页高速缓存），页高速缓存在内核中的结构体就是这个 struct address_space。它被文件的 inode 所持有。

文件页mapping指针的最低位会被设置为0，匿名页mapping指针的最低位会被设置为1。内核可以通过检查这个标记为0还是1来区分是匿名页还是文件页。如果是文件页，address_space对应的mapping为address_space结构体，如果是匿名页mapping是anon_vma结构体。

我们通常所说的内存映射是正向映射，即从虚拟内存到物理内存的映射。而反向映射则是从物理内存到虚拟内存的映射，用于**当某个物理内存页需要进行回收或迁移时**，此时需要去找到这个物理页被映射到了哪些进程的虚拟地址空间中，并断开它们之间的映射。

为什么会有：active和inactive两个链表？解决页面颠簸的问题。

active和inactive两个链表也分为匿名页和文件页两类。swappiness参数用于控制swap机制的积极程度，值越大越倾向于回收匿名页，越低越倾向于回收文件页。将active、inactive链表分成两类后，就能够方便地遍历找到匿名页或文件页进行回收，遍历的数量会少一些。

swappiness参数用于控制回收匿名页的激进程度（a），取值范围为0-100，回收文件页的激进程度为（200-a）。如果配置a=100，那么内核虽然会最激进地回收匿名页，但是依旧也会做文件页的回收。如果配置为0，那么内核不会回收匿名页，仅回收文件页。对于高IO的场景，我们希望少做文件页的回收，则将swappiness配置为100；对于数据库这种倾向于自己管理cache的场景，我们希望内核不要回收匿名页，将swappiness设置为0。

## /sys/kernel/debug/tracing和/sys/kernel/tracing

两者其实是相同的东西，只是`/sys/kernel/debug/tracing`依赖于挂载`debugfs`，而`/sys/kernel/tracing`不依赖于`debugfs`，它属于`tracefs`挂载点。先有的`debugfs`，再有的`tracefs`，`tracefs`存在的原因是解除对`debugfs`的依赖，`/sys/kernel/debug/tracing`当前存在是为了提供后向兼容功能。

## 参数调优

|参数名|作用|使用场景|
|-|-|-|
|vm.dirty_writeback_centisecs|配置定期回写的间隔<br>0：禁用定期回写<br>|平衡内存使用和IO性能。对内存紧张的，使用低回写间隔，提高内存利用率；对IO性能差的使用高回写间隔，避免回写成为性能瓶颈|
|vm.swappiness|回收匿名页的激进程度（a），越高越倾向于回收匿名页，取值范围：[0,100]<br>匿名页:文件页=a:(200-a)<br>0：禁用匿名页回收<br>|对数据库这种自己对内存做管理的业务，最好将vm.swappiness配置为0，不让内核回收匿名页|
