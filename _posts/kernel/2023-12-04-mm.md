---
layout: post
title: "linux 内存管理"
category: kernel
date: 2023-12-04 08:00 +0800
---

内核版本：4.19.289

## 1 虚拟内存、物理内存

### 1.1 内存区域（zone）

* 高端内存（ZONE_HIGH）：高于high_memory地址的部分，最为廉价，又细分为vmalloc区域、持久映射区域、固定映射区域。
* 普通内存（ZONE_NORMAL）：一致的线性映射（对x86来说，如果物理内存超过了896M则无法映射全部的物理内存）。
* DMA内存（ZONE_DMA）：用于外设和系统的数据传输

### 1.X FAQ

#### 内存域的min，low，high

默认情况下，只有当内存域包含页的数目多于zone->page_high的时候，才会分配页，这对应的就是ALLOC_WMARK_HIGH。如果低于zone->page_high的时候还想要分配更多的页，那么就需要添加标记位：ALLOC_WMARK_LOW、ALLOC_WMARK_MIN。
详细参考：深入理解Linux内核架构P178

当内存大小小于page_low的时候，会触发内核的异步回收，内核触发kswapd，直到内存分配的值到达page_high。
当内存大小小于page_min的时候，会触发内核的同步回收，进程被阻塞。page_min可以通过/proc/sys/vm/min_free_kbytes配置。同步回收会阻塞进程，造成长时间的延迟，系统CPU使用率升高，需要尽量避免。
如果同步回收完之后，依旧无法满足分配要求，会触发OOM，内核根据算法选择一个占用物理内存较高的进程杀死。

## 2 rmap

参考：<https://www.cnblogs.com/LoyenWang/p/12164683.html>

参考：<https://richardweiyang-2.gitbook.io/kernel-exploring/00-index/01-anon_rmap_history>

简单总结：

一个物理页对应存在一个anon_vma，这里的anon_vma实际上是一个树结构，可以通过它查找到多个anon_vma。每个anon_vma可以通过avon_vma_chain再查找到对应的vma。

### 2.1 相关的数据结构

#### anon_vma

anon_vma即匿名类型的vma，vma的类型可以有很多种，在rmap中我们重点关注的是匿名类型的vma。一个物理页通过`struct address_space *mapping`直接关联到anon_vma，基于anon_vma通过anon_vma_chain找到属于该anon_vma的vma。

之所以使用anon_vma而不是直接使用vma，是因为vma是动态的，它们可以分裂、融合。

anon_vma是一个树结构，anon_vma在发生缺页异常、fork进程的时候会创建，调用的函数为：anon_vma_alloc()。

#### vma

进程的虚拟地址区，一个进程会有多个虚拟地址区，这些虚拟地址区可能是文件映射、堆、代码段、数据段等等。通过vma找到anon_vma是简单明了的，直接调用vma->anon_vma即可。

#### anon_vma_chain

由于COW机制，一个anon_vma会和多个进程产生联系，此外，每个子进程又存在自己的anon_vma。情况变得复杂起来了。

anon_vma_chain是anon_vma和vma之间的桥梁。最开始的rmap实现中，anon_vma直接等于一个链表，链接多个vma。由于这种实现方案的内存占用过大，后续的rmap实现进行了优化，引入了anon_vma_chain。

这张图描述的比较好：

![img](https://github.com/Geass-LL/draw/raw/master/github-io/rmap-anon-vma-chain.png)

### 2.2 代码说明

#### fork场景

在fork一个子进程时，不仅会创建子进程自己的anon_vma、vma，还会创建一个anon_vma_chain把子进程的vma和父进程anon_vma建立联系。这样做的原因还是COW，确保物理页通过anon_vma能够同时找到父进程和子进程的vma。

anon_vma_fork()

#### 如何将page和anon_vma建立联系？

```c
/**
 * __page_set_anon_rmap - set up new anonymous rmap
 * @page:	Page to add to rmap	
 * @vma:	VM area to add page to.
 * @address:	User virtual address of the mapping	
 * @exclusive:	the page is exclusively owned by the current process
 */
static void __page_set_anon_rmap(struct page *page,
	struct vm_area_struct *vma, unsigned long address, int exclusive)
{
	struct anon_vma *anon_vma = vma->anon_vma;

	BUG_ON(!anon_vma);

	if (PageAnon(page))
		return;

	/*
	 * If the page isn't exclusively mapped into this vma,
	 * we must use the _oldest_ possible anon_vma for the
	 * page mapping!
	 */
	/* 首次的话，这里会是exclusive=true。*/
	if (!exclusive)
		anon_vma = anon_vma->root;

	anon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;
	page->mapping = (struct address_space *) anon_vma;
	page->index = linear_page_index(vma, address);
}
```

## 3 memcg

### 3.1 kernel Document

参考：<https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt>

#### 3.1.1 Design

The core of the design is a counter called the page_counter. The page_counter tracks the current memory usage and limit of the group of processes associated with the controller. Each cgroup has a memory controller specific data structure (mem_cgroup) associated with it.

#### 3.1.2 Accounting

```
                +--------------------+
                |  mem_cgroup        |
                |  (page_counter)    |
                +--------------------+
                 /            ^      \
                /             |       \
           +---------------+  |        +---------------+
           | mm_struct     |  |....    | mm_struct     |
           |               |  |        |               |
           +---------------+  |        +---------------+
                              |
                              + --------------+
                                              |
           +---------------+           +------+--------+
           | page          +---------->  page_cgroup   |
           |               |           |               |
           +---------------+           +---------------+
```

1. Accounting happens per cgroup
2. Each mm_struct knows about which cgroup it belongs to
3. Each page has a pointer to the page_cgroup, which in turn knows the cgroup it belongs to

The accounting is done as follows: mem_cgroup_charge_common() is invoked to set up the necessary data structures and check if the cgroup that is being charged is over its limit. If it is, then reclaim is invoked on the cgroup.
More details can be found in the reclaim section of this document. If everything goes well, a page meta-data-structure called page_cgroup is updated. page_cgroup has its own LRU on cgroup.

* page_cgroup structure is allocated at boot/memory-hotplug time.

#### 3.1.3 Accounting details

All mapped anon pages (RSS) and cache pages (Page Cache) are accounted.

Some pages which are never reclaimable and will not be on the LRU are not accounted. We just account pages under usual VM management.

RSS pages are accounted at page_fault unless they've already been accounted for earlier. A file page will be accounted for as Page Cache when it's inserted into inode (radix-tree). While it's mapped into the page tables of processes, duplicate accounting is carefully avoided.

An RSS page is unaccounted when it's fully unmapped. A PageCache page is unaccounted when it's removed from radix-tree. Even if RSS pages are fully unmapped (by kswapd), they may exist as SwapCache in the system until they are really freed. Such SwapCaches are also accounted.
A swapped-in page is not accounted until it's mapped.

Note: The kernel does swapin-readahead and reads multiple swaps at once.
This means swapped-in pages may contain pages for other tasks than a task causing page fault. So, we avoid accounting at swap-in I/O.

At page migration, accounting information is kept.

Note: we just account pages-on-LRU because our purpose is to control amount of used pages; not-on-LRU pages tend to be out-of-control from VM view.

#### 3.1.4 Shared Page Accounting

Shared pages are accounted on the basis of the first touch approach. The cgroup that first touches a page is accounted for the page. The principle behind this approach is that a cgroup that aggressively uses a shared page will eventually get charged for it (once it is uncharged from the cgroup that brought it in -- this will happen on memory pressure).

But see section 8.2: when moving a task to another cgroup, its pages may be recharged to the new cgroup, if move_charge_at_immigrate has been chosen.

Exception: If CONFIG_MEMCG_SWAP is not used. When you do swapoff and make swapped-out pages of shmem(tmpfs) to be backed into memory in force, charges for pages are accounted against the caller of swapoff rather than the users of shmem.

#### 3.1.5 Swap Extension (CONFIG_MEMCG_SWAP)

Swap Extension allows you to record charge for swap. A swapped-in page is charged back to original page allocator if possible.

When swap is accounted, following files are added.

* memory.memsw.usage_in_bytes.
* memory.memsw.limit_in_bytes.

memsw means memory+swap. Usage of memory+swap is limited by memsw.limit_in_bytes.

Example: Assume a system with 4G of swap. A task which allocates 6G of memory (by mistake) under 2G memory limitation will use all swap. In this case, setting memsw.limit_in_bytes=3G will prevent bad use of swap. By using the memsw limit, you can avoid system OOM which can be caused by swap shortage.

**why 'memory+swap' rather than swap.?**

The global LRU(kswapd) can swap out arbitrary pages. Swap-out means to move account from memory to swap...there is no change in usage of memory+swap. In other words, when we want to limit the usage of swap without affecting global LRU, memory+swap limit is better than just limiting swap from an OS point of view.

**What happens when a cgroup hits memory.memsw.limit_in_bytes?**

When a cgroup hits memory.memsw.limit_in_bytes, it's useless to do swap-out in this cgroup. Then, swap-out will not be done by cgroup routine and file caches are dropped. But as mentioned above, global LRU can do swapout memory from it for sanity of the system's memory management state. You can't forbid it by cgroup.

#### 3.1.6 Reclaim

Each cgroup maintains a per cgroup LRU which has the same structure as global VM. When a cgroup goes over its limit, we first try to reclaim memory from the cgroup so as to make space for the new pages that the cgroup has touched. If the reclaim is unsuccessful, an OOM routine is invoked to select and kill the bulkiest task in the cgroup. (See 10. OOM Control below.)

The reclaim algorithm has not been modified for cgroups, except that pages that are selected for reclaiming come from the per-cgroup LRU list.

NOTE: Reclaim does not work for the root cgroup, since we cannot set any limits on the root cgroup.

Note2: When panic_on_oom is set to "2", the whole system will panic.

When oom event notifier is registered, event will be delivered. (See oom_control section)

### 3.2 charge是什么东西？

参考：<https://www.kernel.org/doc/html/next/admin-guide/cgroup-v1/memcg_test.html>

核心代码：在do_anonymous_page、do_swap_page、do_cow_fault的时候会调用mem_cgroup_try_charge，将page charge到对应的memcg。

try_charge简单理解成记录的意思。try_charge()会调度到执行mem_cgroup_handle_over_high，此时，执行reclaim_high进行页面回收。
`try_charge() -> schedule_work(&memcg->high_work) -> (high_work.func = high_work_func) -> high_work_func -> reclaim_high`。注意这里和page_min有区别，此处是per cgroup的内存回收。

相关函数：mem_cgroup_try_charge, mem_cgroup_commit_charge, mem_cgroup_cancel_charge

## 4 内存分配

### 4.1 malloc

参考：<https://www.xiaolincoding.com/os/3_memory/malloc.html#linux-%E8%BF%9B%E7%A8%8B%E7%9A%84%E5%86%85%E5%AD%98%E5%88%86%E5%B8%83%E9%95%BF%E4%BB%80%E4%B9%88%E6%A0%B7>

### 4.2 伙伴系统

伙伴系统需要考虑反碎片的问题。页分为：不可移动页、可回收页、可移动页三种，相应的有几个迁移类型：

```c
enum migratetype {
	MIGRATE_UNMOVABLE,
	MIGRATE_MOVABLE,
	MIGRATE_RECLAIMABLE,
	MIGRATE_PCPTYPES,	/* the number of types on the pcp lists */
	MIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,
#ifdef CONFIG_CMA
	/*
	 * MIGRATE_CMA migration type is designed to mimic the way
	 * ZONE_MOVABLE works.  Only movable pages can be allocated
	 * from MIGRATE_CMA pageblocks and page allocator never
	 * implicitly change migration type of MIGRATE_CMA pageblock.
	 *
	 * The way to use it is to change migratetype of a range of
	 * pageblocks to MIGRATE_CMA which can be done by
	 * __free_pageblock_cma() function.  What is important though
	 * is that a range of pageblocks must be aligned to
	 * MAX_ORDER_NR_PAGES should biggest page be bigger then
	 * a single pageblock.
	 */
	MIGRATE_CMA,
#endif
#ifdef CONFIG_MEMORY_ISOLATION
	MIGRATE_ISOLATE,	/* can't allocate from here */
#endif
	MIGRATE_TYPES
};
```

kswapd只会回收可回收页。

### 4.3 伙伴系统如何分配页

伙伴系统中分配物理页的api为：alloc_pages，可以通过指定掩码的形式确定到底从哪个内存域分配页。

alloc_pages使用的掩码页指定了内存域分配的决心：

```
#define ALLOC_NO_WATERMARKS 0x01 /* 完全不检查水印 */
#define ALLOC_WMARK_MIN 0x02 /* 使用pages_min水印 */
#define ALLOC_WMARK_LOW 0x04 /* 使用pages_low水印 */
#define ALLOC_WMARK_HIGH 0x08 /* 使用pages_high水印 */
#define ALLOC_HARDER 0x10 /* 试图更努力地分配，即放宽限制 */
#define ALLOC_HIGH 0x20 /* 设置了__GFP_HIGH */
#define ALLOC_CPUSET 0x40 /* 检查内存结点是否对应着指定的CPU集合 */
```

alloc_pages会调用get_page_from_freelist分配页，如果分配失败，alloc_pages会通过修改内存分配标志和降低水印的方法来尝试继续分配。

伙伴系统的相关内容可以学习：<https://www.cnblogs.com/binlovetech/p/17090846.html>

rmqueue正是伙伴系统的核心逻辑：get_page_from_freelist -> rmqueue -> __rmqueue_smallest

### 4.4 slab分配器

slab分配器用于分配一些小块的内存和内核缓存。

* 内存分配函数：kmalloc、__kmalloc、kmalloc_node
* 内核缓存函数：kmem_cache_alloc、kmem_cache_alloc_node

### 4.X 一些内核接口

* 查看伙伴系统大致信息：`/proc/buddyinfo`
* 查看slab的大致信息：`/proc/slabinfo`

#### 复合页

复合页是由物理上连续的多个普通页组成的，在内核的视角中当作是`1`个特殊的内存页对待。

复合页的第一个页称为首页，剩下的所有页称为尾页。尾页都会包含一个指向首页的指针。

在申请完复合页所需的普通页之后，需要按照复合页的逻辑组合这些页。

#### 冷热页

冷热页主要优化单页分配，放在CPU的高速缓存中的叫做热页，否则叫做冷页，冷热页是per CPU的。

pcplist：per_cpu_pageset_list的简写。

## 5 内存回收

参考：<https://segmentfault.com/a/1190000020937950>

### 5.1 页面回收

换页是页面回收的方法之一，并不是所有的物理页都需要换出到块设备，举个例子：（1）写文件的场景，物理页面的后备存储器是块设备，仅需要将物理内存中的数据同步到块设备即可；（2）像是可执行文件，它在物理页面映射的内容是不可变的，可以直接丢弃相关页面。

允许换页的物理内存包含以下几种：

1. 类别为MAP_ANONYMOUS的页，没有关联的文件。
2. 私有映射，映射修改后不能向块设备同步数据的页。
3. malloc分配的页，或者是匿名映射。
4. 实现进程间通信机制的页。

注意：内核本身使用的页不会被换出。s

### 5.2 LRU算法

参考：<https://www.ctyun.cn/zhishi/p-158284>

#### 5.2.1 LRU链表

每个内存节点会包含以下的LRU链表。LRU、伙伴系统针对的是物理内存。

```c
enum lru_list {
	LRU_INACTIVE_ANON = LRU_BASE,
	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,
	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
	LRU_UNEVICTABLE,
	NR_LRU_LISTS
};
```

#### 5.2.2 最简化版的LRU链表

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-simple.png)

新页面会被添加到活跃链表头，随着老化过程，会被移到不活跃链表头，再移动到链表尾，最后被移除，或者重新添加到活跃LRU链表。

加入lru：lru_cache_add。

#### 5.2.3 二次机会法

核心思想： 在链表尾置换页面时，检查页面的访问位，访问位为0，就淘汰；访问位为1，就给它第二次机会同时将访问位清零;如果该页面被再次访问，访问位会置1，这样被频繁使用的页面，访问位总是1，就不会被淘汰。

linux使用PG_active和PG_referenced两个标志位来实现第二次机会法；

* PG_active：表示处于活跃链表；
* PG_referenced：软件记录访问标记(实际硬件访问标记从页表的PTE_YOUNG获取)

**（1）LRU的原始状态**

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-second-chance-1.png)

**（2）新分配一个匿名页，PG_refereced为0**

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-second-chance-2.png)

**（3）访问匿名页**

访问活跃链表的匿名页会将PG_referenced修改为1。

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-second-chance-3.png)

访问不活跃链表的匿名页也会将PG_referenced修改为1。

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-second-chance-4.png)

如果不活跃链表的匿名页PG_referenced已经为1，则修改为0，页面移到活跃链表。

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-second-chance-5.png)

**（4）淘汰页面**

注意，从尾部开始遍历不活跃链表，如果PG_referenced为1，则修改为0。如果已经为0,那么淘汰。

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-second-chance-6.png)

**（5）页面老化：活跃链表迁移到不活跃链表**

活跃链表尾部，如果PG_referenced为0，则迁移到不活跃链表。

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-second-chance-7.png)

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-second-chance-8.png)

#### 5.2.4 状态转换图

![img](https://github.com/Geass-LL/draw/raw/master/github-io/lru-second-chance-9.png)

### 5.X FAQ

#### 什么时候进行内存回收？

alloc_page、pagefault的时候都可以进行内存回收了，当然会进行一些检查，比方说当前空闲的页面数量。

#### 怎么进行内存回收？

主要回收的对象是匿名页和文件页，基于LRU算法。

* 文件页的回收：需要判断页是clean的还是dirty的，如果是dirty的，需要先将页面写入磁盘，再释放内存，这个过程会产生IO操作。
* 匿名页的回收：匿名页无论如何都是要写入磁盘的，因此也会产生IO操作。

页面的回收都会涉及到IO,影响系统的性能。

#### 同步回收和异步回收？

* 同步回收：direct reclaim
* 异步回收：kswapd

两者底层都会调用`shrink_node_memcg -> shrink_list -> shrink_inactive_list`。参考：<http://0fd.org/2019/06/10/cgroup-memory-management-page-cache-reclaim>

每个内存域都存在一个对应的kswapd进程，也有自己的水线。

#### page_min，page_low，page_high

* 如果空闲页多于pages_high，则内存域的状态是理想的。
* 如果空闲页的数目低于pages_low，则内核开始将页换出到硬盘。
* 如果空闲页的数目低于pages_min，那么页回收工作的压力就比较大，因为内存域中急需空闲页。

#### 为什么lowmem_reserve是一个数组？理论上一个zone一个数字不就够了？

高端内存如果不够用了，可以借用低端内存，但是低端内存不能反过来借用。如果高端内存借得太多，就可能导致低端内存自己无法使用了。

`cat /proc/zoneinfo`获取得到zone DMA的protection: (0, 1643, 15674, 15674, 15674)。

|DMA|DMA32|NORMAL|MOVABLE|DEVICE|
|-|-|-|-|-|
|自己的可以放开用|仅当DMA空闲页大于1643时，可以借给DMA32|仅当空闲也大于15674时，可以借给NORMAL|类似|类似|

所以这个数组实际上对于MOVABLE、DEVICE这种本身就是高端内存的，没什么用，反正不会借出去。这里提到的MOVABLE实际上是一个虚拟内存域，联想到前面伙伴系统提到的迁移类型。

<https://zhuanlan.zhihu.com/p/258560892>

## 一些好的blog

<https://www.cnblogs.com/LoyenWang>
