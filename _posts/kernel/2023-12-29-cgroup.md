---
layout: post
date: 2023-12-29 16:00:00 +0800
category: kernel
title: "关于cgroup"
---

## cgroup.c

cgroup.c是linux cgroup框架层的具体代码实现。

## cgroup-v2接口简单介绍

<https://zorrozou.github.io/docs/%E8%AF%A6%E8%A7%A3Cgroup%20V2.html>

## cgroup-v2相比cgroup-v1的变化

<https://www.infoq.cn/article/hbqqfeyqxzhnes5jipqt>

**no internal processes：进程只能绑定到叶子节点，线程不受影响**

## systemd和cgroup

### 如何通过systemd启用cgroup v2

在内核命令行配置参数：`systemd.unified_cgroup_hierarchy=1`。

### 常用的cgroup接口

|接口名称|cgroup版本|说明|
|-|-|-|
|cgroup.controllers|v2|当前cgroup可以配置的子系统|
|cgroup.subtree_control|v2|当前cgroup的子目录默认继承的子系统，subtree_control应该是controllers的子集|
|cgroup.events|v2|如果子cgroup有进程存在，populated为1；否则为0。可以使用inotify监控|
|cgroup.stat|v2|记录cgroup中存活、死亡的子cgroup数量|
|cgroup.max.depth、cgroup.max.descendants|v2|分别表示子cgroup的最大深度、子cgroup的最大数量|
|cgroup.threads|v2|cgroup包含的线程|
|cgroup.pressure|v2|cgroup当前的cpu、io、memory压力|
|cgroup.type|v2|cgroup类型，可能为normal、threaded|
|notify_on_release、release_agent|v1|如果notify_on_release配置为1，则cgroup内进程清空后会调用release_agent脚本|
|cgroup.clone_children|v1|配置为1后，某些子系统的cgroup配置可以继承到子cgroup，例如cpuset的绑核配置|

在切换cgroup-v2后，子系统的cgroup接口有大规模变更。考虑到systemd目前是k8s的默认cgroup驱动，这里也列出各子系统systemd的常用配置。systemd原来面向cgroup-v1的一些`systemd.resource-control`配置已在systemd-252版本标记为弃用，用户的相关配置会被systemd自动转换为对应的cgroup-v2配置。

### cpu子系统接口

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|cpu.cfs_period_us|CPUQuotaPeriodSec|v1|进程执行耗用的CPU时间限额统计周期|
|cpu.cfs_quota_us|CPUQuota|v1|进程执行耗用的CPU时间限额|
|cpu.max|CPUQuota、CPUQuotaPeriodSec|v2|cgroup-v2合并了cpu.cfs_period_us和cpu.cfs_quota_us|
|cpu.shares|CPUShares（废弃）|v1|配置cgroup耗用的CPU时间权重|
|cpu.weight|CPUWeight|v2|配置cgroup耗用的CPU时间权重|
|cpu.idle|CPUWeight|v2|CPUWeight=idle标记cgroup为空闲调度|
|cpuset.cpus|AllowedCPUs|v2|cpuset子系统，允许使用的CPU|

### memory子系统接口

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|memory.limit_in_bytes|MemoryLimit（废弃）|v1|限制cgroup最大可使用的内存|
|memory.memsw.limit_in_bytes|MemoryMemswLimit|v1|限制cgroup最大可使用的内存、交换分区总和（openEuler自研，其他发行商不存在）|
|memory.max|MemoryMax、MemoryLimit|v2|限制cgroup可使用内存的硬限制，超过会触发OOM|
|memory.high|MemoryHigh|v2|限制cgroup可使用内存的软限制，实际使用的内存可以超过该值，但是超过后会触发内核积极回收内存，进程阻塞|
|memory.min|MemoryMin|v2|内存紧张时，保证cgroup最少可用内存的硬限制。内核在内存回收时，强制确保cgroup最少有memory.min的内存可用|
|memory.low|MemoryLow|v2|内存紧张时，保证cgroup最少可用内存的软限制。内核会尽量使cgroup最少有memory.low的内存可用，但不是强制性的，不会完全遵守|
|memory.swap.max|MemorySwapMax|v2|限制cgroup可使用的交换分区上限|
|memory.zswap.max|MemoryZSwapMax|v2|限制cgroup可使用的zswap上线|
|cpuset.mems|AllowedMemoryNodes|v2|cpuset子系统，允许使用的内存节点|

### PID子系统

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|pids.max|TasksMax|v1/v2|cgroup的进程上限|

### BlockIO/IO子系统接口

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|block.weight|BlockIOWeight（废弃）|v1|cgroup的块设备IO使用权重|
|blkio.weight_device|BlockIODeviceWeight（废弃）|v1|具体块设备的IO使用权重|
|blkio.throttle.read_bps_device|BlockIOReadBandwidth（废弃）|v1|具体块设备的读取带宽|
|blkio.throttle.write_bps_device|BlockIOWriteBandwidth（废弃）|v1|具体块设备的写入带宽|
|io.weight|IOWeight、BlockIOWeight、IODeviceWeight、BlockIODeviceWeight|v2|cgroup的全量块设备及具体块设备的IO使用权重|
|io.max|IOReadBandwidthMax、IOWriteBandwidthMax、BlockIOReadBandwidth、BlockIOWriteBandwidth|v2|cgroup的全量块设备及具体块设备的读写带宽|
|io.latency|IODeviceLatencyTargetSec|v2|cgroup具体块设备的IO延迟|

### cpuset and freezer

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|freezer.state|FreezerState|v1|cgroup冻结状态（openEuler自研，其他发行商不存在）|
|cpuset.cpus|CPUSetCpus|v1|允许使用的cpu（openEuler自研，其他发行商不存在）|
|cpuset.mems|CPUSetMems|v1|允许使用的内存节点（openEuler自研，其他发行商不存在）|
|cpuset.clone_children|CPUSetCloneChildren|v1|cpuset的继承行为（openEuler自研，其他发行商不存在）|
|cpuset.memory_migrate|CPUSetMemMigrate|v1|cgroup的内存迁移行为（openEuler自研，其他发行商不存在）|
|cpuset.cpus|AllowedCPUs|v2|允许使用的cpu|
|cpuset.mems|AllowedMemoryNodes|v2|允许使用的内存节点|
|cpuset.cpus.effective|EffectiveCPUs（只读）|v2|生效的cpu|
|cpuset.mems.effective|EffectiveMemoryNodes（只读）|v2|生效的内存节点|

#### systemd为什么不支持cgroup-v1的cpuset、freezer接口？

<https://github.com/systemd/systemd/pull/20580>

```
We communicated this at verious places. New features are only added to the cgroupsv2 logic since quite a while, cgroupsv1 is only supported to the level it is supported to right now.

In particular the two controllers you have been looking into are just broken in cgroupsv1, i.e. cpuset claims to be hierarchial, but is not, and freezer requires busy polling the state file to see when things completed.

Sorry. But this not the way forward: update to cgroupsv2 instead.

Sorry if this is disappointing.
```

### systemd用到的其他cgroup-v2接口

* pids.current，io.stat，memory.current，cpu.stat：systemd-cgtop用于展示cpu、io统计情况
* notify_on_release：监控cgroup的释放
* memory.swap.current，memory.stat：systemd-oomd特性使用
* cgroup.type：systemd-detect-virt特性使用
* cgroup.procs、cgroup.threads、cgroup.controllers、cgroup.events、cgroup.freeze、cgroup.subtree_control：systemd cgroup特性基础能力使用
* cgroup.stat：systemd-nspawn特性使用
* cgroup.max.depth、cgroup.max.descendants：systemd-255版本未使用，systemd TODO中表明后续会使用

### systemd废弃的resource-control接口

systemd-252版本废弃了部分cgroup-v1的`systemd.resource-control`接口。但是用户依旧可以使用这些接口，这些接口在cgroup-v2使用时，systemd会自动进行配置转换，具体的转换方法在下表的最后一列呈现。

|接口名称|作用|替换接口|systemd的v2兼容方案|
|-|-|-|-|
|CPUShares|配置cgroup-v1的cpu.shares，决定不同cgroup的CPU使用时间权重|CPUWeight|cpu.shares * 100 / 1024的值作为cpu.weight|
|StartupCPUShares|同上|StartupCPUWeigh|同上|
|MemoryLimit|配置cgroup-v1的memory.limit_in_bytes|MemoryMax|原始值配置到memory.max|
|BlockIOAccounting|启用单元的blkio子系统|NA|NA|
|BlockIOWeight|配置所有块设备的blkio.weight，blkio.bfq.weight|IOWeight|blkio.weight * 100 / 500的值作为io.weight|
|StartupBlockIOWeight|同上|StartupIOWeight|同上|
|BlockIODeviceWeight|配置特定块设备的blkio.weight，blkio.bfq.weight|IODeviceWeight|同BlockIOWeight|
|BlockIOReadBandwidth|配置特定块设备的读带宽blkio.throttle.read_bps_device|IOReadBandwidthMax|原始值配置到io.max的rbps|
|BlockIOWriteBandwidth|配置特定块设备的写带宽blkio.throttle.write_bps_device|IOWriteBandwidthMax|原始值配置到io.max的wbps|

## PSI

<https://www.cnblogs.com/Linux-tech/p/12961296.html>

全局的PSI值：`/proc/pressure`，per cgroup的PSI值：`/sys/fs/cgroup/{cpu.pressure, io.pressure, memory.pressure}`。

systemd的某些配置会读取这些配置文件，如：`ConditionMemoryPressure`, `ConditionCPUPressure`, `ConditionIOPressure`。systemd-oomd特性会额外读取：`memory.oom.group`，`memory.pressure`。

/proc/pressure/cpu的full行是没有意义的，在5.13内核前不存在该行。后来内核更新新增了该行，并将full行的内容设置为全0。

* `some`：至少有一个任务阻塞在某个资源上的时间占比。
* `full`：所有非idle任务都阻塞在某个资源上的时间占比，此时CPU资源将完全浪费。

每个CPU的PSI状态如下：

|PSI状态|描述|
|-|-|
|PSI_IO_SOME|至少存在一个任务处于iowait状态|
|PSI_IO_FULL|CPU上至少存在一个任务处于iowait状态，并且该CPU没有可执行的程序，进入了idle状态|
|PSI_MEM_SOME|类似，处于memory stall状态|
|PSI_MEM_FULL|类似|
|PSI_CPU_SOME|该CPU至少有一个任务在等待调度|
|PSI_NONIDLE|CPU并非处于完全空闲状态。即：没有任务在等待IO、等待MEM、或者CPU|
