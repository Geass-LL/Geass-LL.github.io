---
layout: post
date: 2023-12-29 16:00:00 +0800
category: kernel
title: "关于cgroup"
---

## cgroup V2接口简单介绍

<https://zorrozou.github.io/docs/%E8%AF%A6%E8%A7%A3Cgroup%20V2.html>

## cgroup V2相比V1的变化

<https://www.infoq.cn/article/hbqqfeyqxzhnes5jipqt>

### 常用的cgroup接口

|接口名称|cgroup版本|说明|
|-|-|-|
|cgroup.controllers|v2|当前cgroup可以配置的子系统|
|cgroup.subtree_control|v2|当前cgroup的子目录默认继承的子系统，subtree_control应该是controllers的子集|
|cgroup.events|v2|如果子cgroup有进程存在，populated为1；否则为0。可以使用inotify监控|
|cgroup.stat|v2|记录cgroup中存活、死亡的子cgroup数量|
|cgroup.max.depth、cgroup.max.descendants|v2|分别表示子cgroup的最大深度、子cgroup的最大数量|
|cgroup.threads|v2|cgroup包含的线程|
|cgroup.pressure|v2|cgroup当前的cpu、io、memory压力|
|cgroup.type|v2|cgroup类型，可能为normal、threaded|
|notify_on_release、release_agent|v1|如果notify_on_release配置为1，则cgroup内进程清空后会调用release_agent脚本|
|cgroup.clone_children|v1|配置为1后，某些子系统的cgroup配置可以继承到子cgroup，例如cpuset的绑核配置|

在切换cgroup v2后，子系统的cgroup接口有大规模变更。考虑到systemd目前是k8s的默认cgroup驱动，这里也列出各子系统systemd的常用配置。systemd原来面向cgroup v1的一些`systemd.resource-control`配置已在systemd-252版本标记为弃用，用户的相关配置会被systemd自动转换为对应的cgroup v2配置。

### 如何通过systemd启用cgroup v2

在内核命令行配置参数：`systemd.unified_cgroup_hierarchy=1`。

### 常用的cpu子系统接口

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|cpu.cfs_period_us|CPUQuotaPeriodSec|v1|进程执行耗用的CPU时间限额统计周期|
|cpu.cfs_quota_us|CPUQuota|v1|进程执行好用的CPU时间限额|
|cpu.max|CPUQuota、CPUQuotaPeriodSec|v2|cgroupV2合并了cpu.cfs_period_us和cpu.cfs_quota_us|
|cpu.shares|CPUShares（废弃）、CPUWeight|v1|配置cgroup耗用的CPU时间权重|
|cpu.weight|CPUShares（废弃）、CPUWeight|v2|配置cgroup耗用的CPU时间权重|

### 常用的memory子系统接口

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|memory.limit_in_bytes|MemoryLimit|v1|限制cgroup最大可使用的内存|
|memory.memsw.limit_in_bytes|MemoryMemswLimit|v1|限制cgroup最大可使用的内存、交换分区总和（openEuler自研，其他发行商不存在）|
|memory.max|MemoryMax、MemoryLimit|v2|限制cgroup可使用内存的硬限制，超过会触发OOM|
|memory.high|MemoryHigh|v2|限制cgroup可使用内存的软限制，实际使用的内存可以超过该值，但是超过后会触发内核积极回收内存，进程阻塞|
|memory.min|MemoryMin|v2|内存紧张时，保证cgroup最少可用内存的硬限制。内核在内存回收时，强制确保cgroup最少有memory.min的内存可用|
|memory.low|MemoryLow|v2|内存紧张时，保证cgroup最少可用内存的软限制。内核会尽量使cgroup最少有memory.low的内存可用，但不是强制性的，不会完全遵守|
|memory.swap.max|MemorySwapMax|v2|限制cgroup可使用的交换分区上限|

### 常用的BlockIO/IO子系统接口

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|block.weight|BlockIOWeight|v1|cgroup的块设备IO使用权重|
|blkio.weight_device|BlockIODeviceWeight|v1|具体块设备的IO使用权重|
|blkio.throttle.read_bps_device|BlockIOReadBandwidth|v1|具体块设备的读取带宽|
|blkio.throttle.write_bps_device|BlockIOWriteBandwidth|v1|具体块设备的写入带宽|
|io.weight|IOWeight、BlockIOWeight、IODeviceWeight、BlockIODeviceWeight|v2|cgroup的全量块设备及具体块设备的IO使用权重|
|io.max|IOReadBandwidthMax、IOWriteBandwidthMax、BlockIOReadBandwidth、BlockIOWriteBandwidth|v2|cgroup的权利块设备及具体块设备的读写带宽|


**其他差异：**

1. no internal processes：进程只能绑定到叶子节点，线程不受影响

## PSI

<https://www.cnblogs.com/Linux-tech/p/12961296.html>

全局的PSI值：`/proc/pressure`，per cgroup的PSI值：`/sys/fs/cgroup/{cpu.pressure, io.pressure, memory.pressure}`

/proc/pressure/cpu的full行是没有意义的，在5.13内核前不存在该行。后来内核更新新增了该行，并将full行的内容设置为全0。

* `some`：至少有一个任务阻塞在某个资源上的时间占比。
* `full`：所有非idle任务都阻塞在某个资源上的时间占比，此时CPU资源将完全浪费。

每个CPU的PSI状态如下：

|PSI状态|描述|
|-|-|
|PSI_IO_SOME|至少存在一个任务处于iowait状态|
|PSI_IO_FULL|CPU上至少存在一个任务处于iowait状态，并且该CPU没有可执行的程序，进入了idle状态|
|PSI_MEM_SOME|类似，处于memory stall状态|
|PSI_MEM_FULL|类似|
|PSI_CPU_SOME|该CPU至少有一个任务在等待调度|
|PSI_NONIDLE|CPU并非处于完全空闲状态。即：没有任务在等待IO、等待MEM、或者CPU|


